# -*- coding: utf-8 -*-
# arabam_scraper_windows.py

import os, sys, time, random
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Tuple, Optional

import requests
from bs4 import BeautifulSoup
from openpyxl import Workbook, load_workbook

# --- Windows konsolda UTF-8 ---
try:
    if os.name == "nt":
        os.system("chcp 65001 >NUL")
        sys.stdout.reconfigure(encoding="utf-8")
except Exception:
    pass

BASE = "https://www.arabam.com"
BASE_LIST = f"{BASE}/ikinci-el/otomobil"

UA = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
)

# â”€â”€ SVG parÃ§a id â†’ ParÃ§a AdÄ± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PART_NAME_MAP = {
    "B0701": "Sol Arka KapÄ±",
    "B0801": "Sol Ã–n KapÄ±",
    "B01201": "Ã–n Tampon",
    "B0301": "Sol Arka Ã‡amurluk",
    "B01101": "Sol Ã–n Ã‡amurluk",
    "B0901": "SaÄŸ Ã–n Ã‡amurluk",
    "B0401": "SaÄŸ Arka KapÄ±",
    "B0501": "SaÄŸ Ã–n KapÄ±",
    "B0101": "SaÄŸ Arka Ã‡amurluk",
    "B01001": "Motor Kaputu",
    "B0201": "Bagaj KapaÄŸÄ±",
    "B0601": "Tavan",
    "B01301": "Arka Tampon",
}

# â”€â”€ Excel kolonlarÄ± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PROPERTY_COLUMNS = [
    "Ä°lan No", "Ä°lan Tarihi", "Marka", "Seri", "Model",
    "YÄ±l", "KM", "Vites", "YakÄ±t Tipi",
    "Kasa Tipi", "Renk", "Motor Hacmi", "Motor GÃ¼cÃ¼",
    "Ã‡ekiÅŸ", "AraÃ§ Durumu", "Boya-deÄŸiÅŸen", "Takas", "Kimden"
]
PART_COLUMNS = list(PART_NAME_MAP.values())
BASE_COLUMNS = ["BaÅŸlÄ±k", "Fiyat", "Ä°l/Ä°lÃ§e", "Link"]
ALL_COLUMNS = BASE_COLUMNS + PROPERTY_COLUMNS + PART_COLUMNS

# â”€â”€ Site etiketleri â†’ Excel kolonu eÅŸlemesi â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SITEKEY_TO_COL = {
    "Ä°lan No": "Ä°lan No",
    "Ä°lan Tarihi": "Ä°lan Tarihi",
    "Marka": "Marka",
    "Seri": "Seri",
    "Model": "Model",
    "YÄ±l": "YÄ±l",
    "Kilometre": "KM",
    "KM": "KM",
    "Vites Tipi": "Vites",
    "Vites": "Vites",
    "YakÄ±t Tipi": "YakÄ±t Tipi",
    "Kasa Tipi": "Kasa Tipi",
    "Renk": "Renk",
    "Motor Hacmi": "Motor Hacmi",
    "Motor GÃ¼cÃ¼": "Motor GÃ¼cÃ¼",
    "Ã‡ekiÅŸ": "Ã‡ekiÅŸ",
    "AraÃ§ Durumu": "AraÃ§ Durumu",
    "Boya-deÄŸiÅŸen": "Boya-deÄŸiÅŸen",
    "Takasa Uygun": "Takas",
    "Takas": "Takas",
    "Kimden": "Kimden",
}

def clean(s: str) -> str:
    return (s or "").replace("\xa0", " ").strip().strip('"').strip()

def map_props_to_columns(props: Dict[str, str]) -> Dict[str, str]:
    out: Dict[str, str] = {}
    for k, v in props.items():
        key = clean(k).rstrip(":")
        col = SITEKEY_TO_COL.get(key)
        if col:
            out[col] = clean(v)
    return out

# â”€â”€ Decoy arama listesi â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DECOY_QUERIES = [
    "bmw 320i", "bmw 520i", "audi a4", "audi a3",
    "mercedes c200", "skoda superb", "volvo s60", "honda civic",
]

# â”€â”€ Bekleme aralÄ±klarÄ± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WAIT_ITEM_RANGE  = (3.0, 7.0)       # ilanlar arasÄ± (sn)
WAIT_PAGE_RANGE  = (60.0, 240.0)    # sayfa geÃ§iÅŸi (sn): 1â€“4 dk
LONG_BREAK_RANGE = (180.0, 300.0)   # uzun mola (sn): 3â€“5 dk

# â”€â”€ HTTP session (tek yerden) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SESSION = requests.Session()
SESSION.headers.update({
    "User-Agent": UA,
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7",
    "Connection": "keep-alive",
    "Cache-Control": "no-cache",
    "Pragma": "no-cache",
})

def build_list_url(query: str) -> str:
    slug = "-".join(query.lower().split())
    return f"{BASE_LIST}/{slug}"

def turkish_day_month(now: datetime) -> str:
    ay_ad = {
        1:"ocak",2:"ÅŸubat",3:"mart",4:"nisan",5:"mayÄ±s",6:"haziran",
        7:"temmuz",8:"aÄŸustos",9:"eylÃ¼l",10:"ekim",11:"kasÄ±m",12:"aralÄ±k"
    }
    return f"{now.day}{ay_ad[now.month]}"

def vehicle_token_from_query(q: str) -> str:
    return (q or "").strip().lower().split()[-1]

def fetch_html(url: str, retries: int = 2, timeout: int = 30) -> BeautifulSoup:
    last_err = None
    for attempt in range(retries + 1):
        try:
            r = SESSION.get(url, timeout=timeout)
            r.raise_for_status()
            return BeautifulSoup(r.text, "html.parser")
        except Exception as e:
            last_err = e
            # basit backoff
            time.sleep(1.5 * (attempt + 1))
    # son hata
    raise last_err

def append_row_to_xlsx(path: Path, row: Dict[str, str], columns: List[str]):
    if not path.exists():
        wb = Workbook()
        ws = wb.active
        ws.append(columns)
        wb.save(path)
    wb = load_workbook(path)
    ws = wb.active
    ws.append([row.get(c, "") for c in columns])
    wb.save(path)

# ================= Liste sayfasÄ± parse =================
def extract_listing_rows(list_soup: BeautifulSoup):
    table = list_soup.select_one("table#main-listing")
    if not table: return []
    tbody = table.find("tbody")
    if not tbody: return []
    return tbody.select("tr[id^='listing']")

def extract_listing_link_from_tr(tr) -> Optional[str]:
    a = tr.select_one("div.fade-out-content-wrapper a") or tr.find("a", href=True)
    if not a or not a.get("href"):
        return None
    href = a["href"]
    return href if href.startswith("http") else (BASE + href)

def find_next_page_url(list_soup: BeautifulSoup) -> Optional[str]:
    next_a = (list_soup.select_one("ul.pagination li a#pagingNext") or
              list_soup.select_one("ul.pagination li a#paging_next") or
              list_soup.select_one("a[rel='next']"))
    if next_a and next_a.get("href"):
        href = next_a["href"]
        return href if href.startswith("http") else (BASE + href)
    return None

# ================= Detay sayfasÄ± parse =================
def parse_title_and_price(detail_soup: BeautifulSoup) -> Tuple[str, str]:
    title = ""
    price = ""
    t_el = detail_soup.select_one("h1.product-name") or detail_soup.select_one("h1")
    p_el = (detail_soup.select_one(".product-price")
            or detail_soup.select_one("div.price")
            or detail_soup.select_one("span.price"))
    if t_el: title = clean(" ".join(t_el.stripped_strings))
    if p_el: price = clean(" ".join(p_el.stripped_strings))
    return title, price

def parse_location(detail_soup: BeautifulSoup) -> str:
    # Gerekirse geliÅŸtirilebilir (breadcrumbs veya ilan bilgilerinden Ã§ekilebilir)
    return ""

def parse_property_items(detail_soup: BeautifulSoup) -> Dict[str, str]:
    root = detail_soup.select_one("div.product-properties")
    if not root: return {}
    container = root.select_one("div.product-properties-details") or root
    items = container.select("div.property-item")
    out: Dict[str, str] = {}
    for it in items:
        k_el = it.select_one("div.property-key")
        v_el = it.select_one("div.property-value")
        key = clean(" ".join(k_el.stripped_strings)) if k_el else ""
        val = clean(" ".join(v_el.stripped_strings)) if v_el else ""
        if key:
            out[key] = val
    return out

def normalize_status(s: str) -> str:
    s = clean(s)
    mapping = {
        "Orijinal": "Orijinal",
        "BoyanmÄ±ÅŸ": "BoyalÄ±",
        "BoyalÄ±": "BoyalÄ±",
        "Lokal BoyalÄ±": "Lokal BoyalÄ±",
        "DeÄŸiÅŸmiÅŸ": "DeÄŸiÅŸmiÅŸ",
        "BelirtilmemiÅŸ": "BelirtilmemiÅŸ",
    }
    return mapping.get(s, s)

def parse_damage_map(detail_soup: BeautifulSoup) -> Dict[str, str]:
    root = detail_soup.select_one("#tab-damage-information .damage-information-container svg")
    if not root:
        root = (detail_soup.select_one("div.damage-information-container svg")
                or detail_soup.select_one("svg.db")
                or detail_soup.select_one("svg"))
    if not root:
        return {}
    # Tooltip attribute'larÄ± farklÄ± olabilir: uib-tooltip / data-original-title / title
    elements = root.select("[id]")
    parts: Dict[str, str] = {}
    for el in elements:
        pid = el.get("id") or ""
        status = (el.get("uib-tooltip")
                  or el.get("data-original-title")
                  or el.get("title")
                  or "")
        status = normalize_status(status)
        if not pid or not status:
            continue
        name = PART_NAME_MAP.get(pid, pid)
        parts[name] = status
    return parts

# ================= Decoy ziyaretleri ====================
def visit_decoy_pages(n: int = 1):
    k = max(1, min(2, n))
    chosen = random.sample(DECOY_QUERIES, k=k)
    print(f"ðŸŽ­ Decoy ziyaretleri: {chosen}")
    for q in chosen:
        url = build_list_url(q)
        try:
            _ = fetch_html(url)
            wait = random.uniform(3.0, 8.0)
            print(f"   â€¢ {url} â€” bekleme {wait:.1f}s")
            time.sleep(wait)
        except Exception as e:
            print(f"   â€¢ Decoy hata ({type(e).__name__}): {e}")

# ================= Orkestra =============================
def scrape(query: str, max_pages: Optional[int] = None):
    list_url = build_list_url(query)

    # Dosya adÄ±: {model}_{gÃ¼n+ay}.xlsx  (Ã¶rn. superb_11eylÃ¼l.xlsx)
    now = datetime.now()
    veh = vehicle_token_from_query(query)
    dtr = turkish_day_month(now)
    out_path = Path(f"{veh}_{dtr}.xlsx")
    print(f"ðŸ“„ Ã‡Ä±ktÄ± dosyasÄ±: {out_path}")

    current_url = list_url
    page_idx = 0

    pages_since_decoy = 0
    decoy_interval = random.randint(1, 2)       # 1â€“2 sayfada bir decoy
    next_long_break_at = random.randint(8, 12)  # 8â€“12 sayfada bir uzun mola

    while True:
        page_idx += 1
        print(f"\nðŸ“„ SAYFA {page_idx}: {current_url}")
        try:
            list_soup = fetch_html(current_url)
        except Exception as e:
            print(f"âš  Liste sayfasÄ± hata ({type(e).__name__}): {e}")
            break

        trs = extract_listing_rows(list_soup)
        if not trs:
            print("âš  Ä°lan satÄ±rÄ± bulunamadÄ±; duruyorum.")
            break

        # Sayfadaki ilanlarÄ± rastgele sÄ±rayla iÅŸle
        order = list(range(len(trs)))
        random.shuffle(order)

        for pos in order:
            tr = trs[pos]
            link = extract_listing_link_from_tr(tr)
            if not link:
                continue

            print(f"âž¡ Ä°lan: {link}")
            try:
                detail_soup = fetch_html(link)
            except Exception as e:
                print(f"   â€¢ Detay hata ({type(e).__name__}): {e}")
                continue

            title, price = parse_title_and_price(detail_soup)
            props_raw = parse_property_items(detail_soup)
            props = map_props_to_columns(props_raw)
            parts = parse_damage_map(detail_soup)

            # Excel satÄ±rÄ±
            row: Dict[str, str] = {
                "BaÅŸlÄ±k": title,
                "Fiyat": price,
                "Ä°l/Ä°lÃ§e": parse_location(detail_soup),
                "Link": link,
            }
            for col in PROPERTY_COLUMNS:
                row[col] = props.get(col, "")

            for pname in PART_COLUMNS:
                row[pname] = parts.get(pname, "")

            try:
                append_row_to_xlsx(out_path, row, ALL_COLUMNS)
                print(f"ðŸ’¾ Kaydedildi: {row['BaÅŸlÄ±k'] or '(baÅŸlÄ±k yok)'}")
            except Exception as e:
                print(f"   â€¢ Excel yazma hatasÄ± ({type(e).__name__}): {e}")

            t = random.uniform(*WAIT_ITEM_RANGE)
            print(f"â³ Ä°lan arasÄ± bekleme: {t:.1f}s")
            time.sleep(t)

        # â€” sayfa sonu: decoy & uzun mola â€”
        pages_since_decoy += 1
        if pages_since_decoy >= decoy_interval:
            visit_decoy_pages(n=random.randint(1, 2))
            pages_since_decoy = 0
            decoy_interval = random.randint(1, 2)

        if page_idx >= next_long_break_at:
            long_wait = random.uniform(*LONG_BREAK_RANGE)
            print(f"ðŸ›‹ Uzun mola: {long_wait/60:.1f} dk")
            time.sleep(long_wait)
            next_long_break_at = page_idx + random.randint(8, 12)

        # sonraki sayfa?
        next_url = find_next_page_url(list_soup)
        if not next_url:
            print("ðŸ Sonraki sayfa yok, bitti.")
            break

        if max_pages and page_idx >= max_pages:
            print("ðŸ”š max_pages sÄ±nÄ±rÄ±na ulaÅŸÄ±ldÄ±.")
            break

        page_wait = random.uniform(*WAIT_PAGE_RANGE)
        print(f"ðŸ˜´ Sayfa geÃ§iÅŸi bekleme: {page_wait/60:.1f} dk")
        time.sleep(page_wait)
        current_url = next_url

    print(f"\nâœ… TamamlandÄ±. Excel: {out_path}")

# ================= Ã‡ALIÅžTIR =============================
if __name__ == "__main__":
    if len(sys.argv) >= 2:
        q = sys.argv[1]
        try:
            max_pages = int(sys.argv[2]) if len(sys.argv) >= 3 else None
        except ValueError:
            max_pages = None
    else:
        q = input("Marka + model (Ã¶rn: 'skoda superb'): ").strip()
        max_pages = None

    scrape(q, max_pages=max_pages)
