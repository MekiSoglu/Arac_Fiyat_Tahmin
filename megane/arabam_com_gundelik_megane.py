# -*- coding: utf-8 -*-
# arabam_com_gundelik_playwright_hybrid.py
#
# - Playwright ile ilk sayfadan canonical + ?sort=startedAt.desc alÄ±nÄ±r
# - Requests ile tÃ¼m sayfalar Ã§ekilir, ilanlar SIRAYLA iÅŸlenir, 3â€“7 sn bekleme
# - Hedef gÃ¼n mantÄ±ÄŸÄ± (bugÃ¼n/dÃ¼n/ay-gÃ¼n), eski ilk ilanda durur
# - Detay sayfasÄ±ndan property ve hasar haritasÄ± (uib-tooltip) okunur
# - Hasar haritasÄ±nda "BelirtilmemiÅŸ"/boÅŸ -> "Orijinal"
# - Ã‡IKTI 1 (gÃ¼nlÃ¼k): C:\Users\EXCALIBUR\Desktop\sahibinden\gundelik_{model}_ilanlari\{model}_{gÃ¼nay}.xlsx
# - Ã‡IKTI 2 (SABÄ°T ÅABLON): C:\Users\EXCALIBUR\Desktop\sahibinden\gundelik_passat_ilanlari\full-passat.xlsx
#   * Ä°ki dosyaya da, KM ve YÄ±l filtreleri SADECE GEÃ‡EN satÄ±rlar eklenir
#   * "Link"e gÃ¶re tekilleÅŸtirme (aynÄ± ilan ikinci kez yazÄ±lmaz)
#   * Åablon dosyada sadece var olan baÅŸlÄ±klara gÃ¶re yazÄ±lÄ±r

import re, time, argparse, random
from pathlib import Path
from typing import Optional, Dict, List, Tuple, Set
from datetime import datetime, timedelta
from urllib.parse import urlparse, parse_qsl, urlencode, urlunparse

import requests
from bs4 import BeautifulSoup
from openpyxl import Workbook, load_workbook
from playwright.sync_api import sync_playwright

# ========= Ayarlar =========
BASE      = "https://www.arabam.com"
BASE_LIST = f"{BASE}/ikinci-el/otomobil"
UA = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
      "(KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36")

HEADLESS = False
DEBUG    = True

FORCE_TEST_DATE = ""  # "2025-09-11" veya "11.09.2025" (boÅŸsa sistem gÃ¼nÃ¼)

def log(*a):
    if DEBUG: print("[DBG]", *a, flush=True)

# ========= SÃ¼tunlar / MAP =========
PART_NAME_MAP = {
    "B0701": "Sol Arka KapÄ±","B0801": "Sol Ã–n KapÄ±","B01201": "Ã–n Tampon",
    "B0301": "Sol Arka Ã‡amurluk","B01101": "Sol Ã–n Ã‡amurluk","B0901": "SaÄŸ Ã–n Ã‡amurluk",
    "B0401": "SaÄŸ Arka KapÄ±","B0501": "SaÄŸ Ã–n KapÄ±","B0101": "SaÄŸ Arka Ã‡amurluk",
    "B01001": "Motor Kaputu","B0201": "Bagaj kapaÄŸÄ±","B0601": "Tavan","B01301": "Arka Tampon",
}
PROPERTY_COLUMNS = [
    "Ä°lan No","Ä°lan Tarihi","Marka","Seri","Model","YÄ±l","KM","Vites","YakÄ±t Tipi",
    "Kasa Tipi","Renk","Motor Hacmi","Motor GÃ¼cÃ¼","Ã‡ekiÅŸ","AraÃ§ Durumu",
    "Boya-deÄŸiÅŸen","Takas","Kimden"
]
PART_COLUMNS = list(PART_NAME_MAP.values())
BASE_COLUMNS = ["BaÅŸlÄ±k","Fiyat","Ä°l/Ä°lÃ§e","Link"]
ALL_COLUMNS  = BASE_COLUMNS + PROPERTY_COLUMNS + PART_COLUMNS

SITEKEY_TO_COL = {
    "Ä°lan No":"Ä°lan No","Ä°lan Tarihi":"Ä°lan Tarihi","Marka":"Marka",
    "Seri":"Seri","Model":"Model","YÄ±l":"YÄ±l",
    "Kilometre":"KM","KM":"KM",
    "Vites Tipi":"Vites","Vites":"Vites",
    "YakÄ±t Tipi":"YakÄ±t Tipi","Kasa Tipi":"Kasa Tipi","Renk":"Renk",
    "Motor Hacmi":"Motor Hacmi","Motor GÃ¼cÃ¼":"Motor GÃ¼cÃ¼","Ã‡ekiÅŸ":"Ã‡ekiÅŸ",
    "AraÃ§ Durumu":"AraÃ§ Durumu","Boya-deÄŸiÅŸen":"Boya-deÄŸiÅŸen",
    "Takasa Uygun":"Takas","Takas":"Takas","Kimden":"Kimden",
}

TR_MONTHS = "ocak ÅŸubat mart nisan mayÄ±s haziran temmuz aÄŸustos eylÃ¼l ekim kasÄ±m aralÄ±k".split()
DATE_PAT  = re.compile(r"\b(\d{1,2})\s+(ocak|ÅŸubat|mart|nisan|mayÄ±s|haziran|temmuz|aÄŸustos|eylÃ¼l|ekim|kasÄ±m|aralÄ±k)(?:\s+\d{4})?\b", re.I)

# ========= YardÄ±mcÄ±lar =========
def clean(s: Optional[str]) -> str:
    return (s or "").replace("\xa0"," ").strip().strip('"')

def parse_today_arg(s: Optional[str]) -> Optional[datetime]:
    if not s: return None
    s = s.strip()
    for fmt in ("%Y-%m-%d","%d.%m.%Y"):
        try: return datetime.strptime(s, fmt)
        except: pass
    raise ValueError("--today: YYYY-MM-DD ya da DD.MM.YYYY")

def resolved_today(today_arg: Optional[str]) -> datetime:
    if FORCE_TEST_DATE:
        return parse_today_arg(FORCE_TEST_DATE) or datetime.now()
    return parse_today_arg(today_arg) or datetime.now()

def build_list_url(query: str) -> str:
    slug = "-".join(query.lower().split())
    return f"{BASE_LIST}/{slug}"

def turkish_day_month(dt: datetime) -> str:
    ay = {1:"ocak",2:"ÅŸubat",3:"mart",4:"nisan",5:"mayÄ±s",6:"haziran",7:"temmuz",8:"aÄŸustos",9:"eylÃ¼l",10:"ekim",11:"kasÄ±m",12:"aralÄ±k"}
    return f"{dt.day}{ay[dt.month]}"

def ensure_parent_dir(path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)

def fetch_html(url: str) -> BeautifulSoup:
    r = requests.get(url, headers={
        "User-Agent": UA,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "tr-TR,tr;q=0.9",
        "Cache-Control":"no-cache","Pragma":"no-cache",
    }, timeout=30)
    r.raise_for_status()
    return BeautifulSoup(r.text, "html.parser")

def to_number(s: str) -> float:
    s = clean(s).replace(".","").replace(",",".")
    s = re.sub(r"[^0-9\.]","", s)
    try: return float(s) if s else float("nan")
    except: return float("nan")

def year_from_text(s: str) -> Optional[int]:
    m = re.search(r"\b(19|20)\d{2}\b", clean(s))
    return int(m.group(0)) if m else None

def should_keep_row(row: Dict[str,str], max_km: Optional[int], min_year: Optional[int]) -> bool:
    """SatÄ±r, KM ve YÄ±l filtrelerini geÃ§iyorsa True."""
    if max_km is not None:
        km_val = to_number(row.get("KM",""))
        if km_val == km_val and km_val > max_km:  # NaN deÄŸil ve sÄ±nÄ±rdan bÃ¼yÃ¼k
            return False
    if min_year is not None:
        y = year_from_text(row.get("YÄ±l",""))
        if y is not None and y < min_year:
            return False
    return True

def read_headers(path: Path) -> List[str]:
    """Var olan dosyanÄ±n baÅŸlÄ±klarÄ±nÄ± sÄ±rayla dÃ¶ndÃ¼rÃ¼r."""
    wb = load_workbook(path, read_only=True)
    ws = wb.active
    headers = [c.value if c.value is not None else "" for c in ws[1]]
    wb.close()
    return headers

def append_row_unique(path: Path, row: Dict[str,str], columns: List[str], unique_key: str = "Link"):
    """DosyayÄ± yoksa oluÅŸturur. 'unique_key' (Link) zaten varsa eklemez."""
    ensure_parent_dir(path)
    if not path.exists():
        wb = Workbook(); ws = wb.active; ws.append(columns); wb.save(path)

    wb = load_workbook(path)
    ws = wb.active
    headers = [c.value for c in ws[1]]
    try:
        key_idx = headers.index(unique_key) + 1
    except ValueError:
        key_idx = None

    existing: Set[str] = set()
    if key_idx:
        for r in range(2, ws.max_row + 1):
            existing.add(str(ws.cell(r, key_idx).value or ""))

    link_val = str(row.get(unique_key,""))
    if key_idx and link_val in existing:
        wb.close()
        return  # zaten var

    ws.append([row.get(c,"") for c in columns])
    wb.save(path); wb.close()

def append_row_to_existing_template(path: Path, row: Dict[str,str], unique_key: str = "Link"):
    """
    Var olan bir ÅŸablon dosyaya (oluÅŸturmadan) yaz.
    BaÅŸlÄ±klarÄ± ÅŸablondan okur; sadece o baÅŸlÄ±klara karÅŸÄ±lÄ±k gelen deÄŸerleri yazar.
    """
    if not path.exists():
        print(f"âš  Åablon bulunamadÄ±, atlanÄ±yor: {path}")
        return

    wb = load_workbook(path)
    ws = wb.active
    headers = [c.value for c in ws[1]]

    # Uniq kontrol
    try:
        key_idx = headers.index(unique_key) + 1
    except ValueError:
        key_idx = None

    existing: Set[str] = set()
    if key_idx:
        for r in range(2, ws.max_row + 1):
            existing.add(str(ws.cell(r, key_idx).value or ""))

    link_val = str(row.get(unique_key,""))
    if key_idx and link_val in existing:
        wb.close()
        return  # zaten var

    # Sadece ÅŸablondaki kolon sÄ±rasÄ±na gÃ¶re yaz
    ws.append([row.get(h,"") for h in headers])
    wb.save(path); wb.close()

# ========= Tarih Ã§Ã¶zÃ¼mleme =========
def date_from_text(text: str, reference_now: datetime) -> Optional[datetime.date]:
    t = (text or "").strip().lower()
    if not t: return None
    if "bugÃ¼n" in t or "saat Ã¶nce" in t or "dk Ã¶nce" in t:
        return reference_now.date()
    if "dÃ¼n" in t:
        return (reference_now - timedelta(days=1)).date()
    m = DATE_PAT.search(t)
    if m:
        gun = int(m.group(1)); ay_ad = m.group(2).lower()
        try: ay = TR_MONTHS.index(ay_ad)+1
        except ValueError: return None
        y_m = re.search(r"\b(\d{4})\b", t)
        yil = int(y_m.group(1)) if y_m else reference_now.year
        try: return datetime(yil, ay, gun).date()
        except ValueError: return None
    return None

def is_on_or_after_target(text: str, target_day: datetime, reference_now: datetime) -> Optional[bool]:
    d = date_from_text(text, reference_now)
    if d is None: return None
    return d >= target_day.date()

# ========= Liste / Detay parse =========
def parse_list_date_from_tr(tr) -> str:
    a = tr.select_one("td.listing-text.tac div.fade-out-content-wrapper a[href]")
    if a: return " ".join(a.stripped_strings).strip()
    t = tr.select_one("time")
    if t: return " ".join(t.stripped_strings).strip()
    for td in tr.select("td"):
        txt = " ".join(td.stripped_strings).strip().lower()
        if any(k in txt for k in ["bugÃ¼n","dÃ¼n","saat Ã¶nce","dk Ã¶nce"]): return txt
        if any(m in txt for m in TR_MONTHS): return txt
    return ""

def extract_listing_link_from_tr(tr) -> Optional[str]:
    a = tr.select_one("td.listing-text.tac div.fade-out-content-wrapper a[href]") \
        or tr.select_one("div.fade-out-content-wrapper a[href]") \
        or tr.find("a", href=True)
    if not a or not a.get("href"): return None
    href = a["href"]
    return href if href.startswith("http") else (BASE + href)

def extract_listing_rows_from_html(html: str):
    soup = BeautifulSoup(html, "html.parser")
    table = soup.select_one("table#main-listing")
    tbody = table.find("tbody") if table else None
    return [] if not tbody else tbody.select("tr[id^='listing']")

def parse_property_items(detail_soup: BeautifulSoup) -> Dict[str,str]:
    root = detail_soup.select_one("div.product-properties")
    if not root: return {}
    cont = root.select_one("div.product-properties-details") or root
    items = cont.select("div.property-item"); out={}
    for it in items:
        k = it.select_one("div.property-key"); v = it.select_one("div.property-value")
        key = clean(" ".join(k.stripped_strings)) if k else ""
        val = clean(" ".join(v.stripped_strings)) if v else ""
        if key: out[key]=val
    return out

def parse_title_and_price(detail_soup: BeautifulSoup) -> Tuple[str,str]:
    t_el = detail_soup.select_one("h1.product-name") or detail_soup.select_one("h1")
    p_el = detail_soup.select_one(".product-price") or detail_soup.select_one("div.price") or detail_soup.select_one("span.price")
    return clean(" ".join(t_el.stripped_strings)) if t_el else "", clean(" ".join(p_el.stripped_strings)) if p_el else ""

def normalize_status(s: str) -> str:
    t = clean(s)
    if not t or t.lower().startswith("belirtilmemiÅŸ"):
        return "Orijinal"
    mapping = {
        "Orijinal": "Orijinal",
        "BoyanmÄ±ÅŸ": "BoyalÄ±",
        "BoyalÄ±": "BoyalÄ±",
        "Lokal BoyalÄ±": "Lokal BoyalÄ±",
        "DeÄŸiÅŸmiÅŸ": "DeÄŸiÅŸmiÅŸ",
    }
    return mapping.get(t, t)

def parse_damage_map(detail_soup: BeautifulSoup) -> Dict[str,str]:
    root = (detail_soup.select_one("#tab-damage-information .damage-information-container svg")
            or detail_soup.select_one("div.damage-information-container svg")
            or detail_soup.select_one("svg"))
    if not root: return {}
    parts={}
    for el in root.select("[id][uib-tooltip]"):
        pid = el.get("id") or ""; status = normalize_status(el.get("uib-tooltip") or "")
        if not pid or not status: continue
        name = PART_NAME_MAP.get(pid, pid); parts[name]=status
    return parts

def parse_location(detail_soup: BeautifulSoup) -> str:
    return ""

# ========= UI: ilk sayfa aÃ§, canonical yakala =========
def try_close_cookie_banners(page):
    selectors = [
        "#onetrust-accept-btn-handler","button#onetrust-accept-btn-handler",
        "button:has-text('Kabul')","button:has-text('TÃ¼mÃ¼nÃ¼ Kabul Et')",
        "button:has-text('Kabul Et')","button:has-text('Accept')",
        "text=TÃ¼mÃ¼nÃ¼ kabul et","text=Kabul Et",
    ]
    for sel in selectors:
        try:
            page.wait_for_selector(sel, timeout=3000)
            page.click(sel)
            log("Cookie banner kapatÄ±ldÄ±:", sel)
            return True
        except Exception:
            continue
    return False

def get_sorted_base_url_via_ui(query: str) -> Tuple[str, str]:
    list_url = build_list_url(query)
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=HEADLESS)
        context = browser.new_context(user_agent=UA, java_script_enabled=True)
        page = context.new_page()
        log("Sayfa aÃ§Ä±lÄ±yor:", list_url)
        page.goto(list_url, timeout=60000, wait_until="domcontentloaded")
        try_close_cookie_banners(page)
        try:
            canonical = page.eval_on_selector("link[rel='canonical']", "el => el.href")
        except Exception:
            canonical = None
        if not canonical:
            canonical = page.url
        purl = urlparse(canonical)
        qs = dict(parse_qsl(purl.query, keep_blank_values=True))
        qs["sort"] = "startedAt.desc"
        new_q = urlencode(qs, doseq=True)
        resolved = urlunparse((purl.scheme, purl.netloc, purl.path, "", new_q, ""))
        html = page.content()
        browser.close()
        return html, resolved

# ========= Orkestra =========
def scrape(query: str, max_km: Optional[int], min_year: Optional[int],
           today_arg: Optional[str], max_pages: Optional[int]=None):
    target_day = resolved_today(today_arg)

    # Model token ve gÃ¼nlÃ¼k Ã§Ä±ktÄ± yolu
    model = query.split()[-1].lower()
    day_mon = turkish_day_month(target_day)

    # GÃ¼nlÃ¼k dosya: C:\Users\EXCALIBUR\Desktop\sahibinden\gundelik_{model}_ilanlari\{model}_{gÃ¼nay}.xlsx
    daily_dir  = Path(r"C:\Users\EXCALIBUR\Desktop\sahibinden\megane") / f"gundelik_{model}_ilanlarÄ±"
    daily_path = daily_dir / f"{model}_{day_mon}.xlsx"

    # SABÄ°T ÅABLON (mevcut olmalÄ±): C:\Users\EXCALIBUR\Desktop\sahibinden\gundelik_passat_ilanlari\full-passat.xlsx
    fixed_full_path = Path(r"C:\Users\EXCALIBUR\Desktop\sahibinden\megane\gundelik_megane_ilanlarÄ±\megane_12eyluÌˆl.xlsx")

    print(f"ğŸ“„ GÃ¼nlÃ¼k: {daily_path}")
    print(f"ğŸ“¦ Åablon (sabit): {fixed_full_path}")

    # Ä°lk sayfa: UI â†’ canonical â†’ sorted base url
    _, base_sorted_url = get_sorted_base_url_via_ui(query)
    print(f"ğŸ”— Taban URL (sorted): {base_sorted_url}")

    total_kept = 0
    page_idx = 0
    stop_all = False

    session = requests.Session()
    session.headers.update({
        "User-Agent": UA,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "tr-TR,tr;q=0.9",
        "Cache-Control":"no-cache","Pragma":"no-cache",
    })

    def page_url(n: int) -> str:
        p = urlparse(base_sorted_url)
        qs = dict(parse_qsl(p.query, keep_blank_values=True))
        qs["sort"] = "startedAt.desc"
        if n > 1:
            qs["page"] = str(n)
        else:
            qs.pop("page", None)
        new_q = urlencode(qs, doseq=True)
        return urlunparse((p.scheme, p.netloc, p.path, "", new_q, ""))

    while True:
        page_idx += 1
        if max_pages and page_idx > max_pages:
            print("ğŸ”š max_pages sÄ±nÄ±rÄ±.")
            break

        url = page_url(page_idx)
        log("GET", url)
        r = session.get(url, timeout=30)
        if r.status_code != 200:
            print(f"âš  Sayfa {page_idx} getirilemedi (HTTP {r.status_code}).")
            break
        html = r.text

        print(f"\nğŸ“„ SAYFA {page_idx}: {url}")
        trs = extract_listing_rows_from_html(html)
        log("Liste satÄ±r sayÄ±sÄ±:", len(trs))
        if not trs:
            print("âš  Ä°lan satÄ±rÄ± bulunamadÄ±; duruyorum.")
            break

        for tr in trs:
            list_date = parse_list_date_from_tr(tr)
            cmp = is_on_or_after_target(list_date, target_day=target_day, reference_now=datetime.now())

            if cmp is False:
                print(f"â›” Hedef gÃ¼nden eski ilan: '{list_date}'. Tarama bitiriliyor.")
                stop_all = True
                break

            link = extract_listing_link_from_tr(tr)
            if not link:
                continue

            # Gerekirse detayda tarih doÄŸrulamasÄ±
            if cmp is None:
                try:
                    detail_check = fetch_html(link)
                except Exception as e:
                    print(f"âš  Detay alÄ±namadÄ± ({e}). AtlanÄ±yor.")
                    continue
                det_props_raw = parse_property_items(detail_check)
                det_props = {}
                for k, v in det_props_raw.items():
                    col = SITEKEY_TO_COL.get(clean(k).rstrip(":"))
                    if col: det_props[col] = clean(v)
                det_date_text = det_props.get("Ä°lan Tarihi","")
                cmp_det = is_on_or_after_target(det_date_text, target_day=target_day, reference_now=datetime.now())
                if cmp_det is False:
                    print(f"â›” Detayda hedef gÃ¼nden eski: '{det_date_text}'. Tarama bitiriliyor.")
                    stop_all = True
                    break
                elif cmp_det is None:
                    print(f"âš  Detay tarih Ã§Ã¶zÃ¼lemedi ('{det_date_text}'). AtlanÄ±yor.")
                    continue

                title, price = parse_title_and_price(detail_check)
                parts = parse_damage_map(detail_check)
                row = {"BaÅŸlÄ±k": title, "Fiyat": price, "Ä°l/Ä°lÃ§e": parse_location(detail_check), "Link": link}
                for c in PROPERTY_COLUMNS: row[c] = det_props.get(c, "")
                for pn in PART_COLUMNS:     row[pn] = parts.get(pn, "")

                # --- filtre + yaz ---
                if should_keep_row(row, max_km, min_year):
                    # GÃ¼nlÃ¼k dosyaya (gerekirse oluÅŸturur)
                    append_row_unique(daily_path, row, ALL_COLUMNS, unique_key="Link")
                    # SABÄ°T ÅABLONA (oluÅŸturmadan, baÅŸlÄ±ÄŸa gÃ¶re eÅŸleÅŸtirerek)
                    append_row_to_existing_template(fixed_full_path, row, unique_key="Link")
                    total_kept += 1
                    print(f"ğŸ’¾ Kaydedildi (detay-doÄŸrulandÄ±): {row['BaÅŸlÄ±k'] or '(baÅŸlÄ±k yok)'}")
                else:
                    print("ğŸš« Filtre nedeniyle eklenmedi.")
                time.sleep(random.uniform(3.0, 7.0))
                continue

            # cmp True: normal akÄ±ÅŸ
            try:
                detail_soup = fetch_html(link)
            except Exception as e:
                print(f"   â€¢ Detay hata: {e}")
                continue

            title, price = parse_title_and_price(detail_soup)
            props_raw = parse_property_items(detail_soup)
            props = {}
            for k, v in props_raw.items():
                col = SITEKEY_TO_COL.get(clean(k).rstrip(":"))
                if col: props[col] = clean(v)
            parts = parse_damage_map(detail_soup)

            row = {"BaÅŸlÄ±k": title, "Fiyat": price, "Ä°l/Ä°lÃ§e": parse_location(detail_soup), "Link": link}
            for c in PROPERTY_COLUMNS: row[c] = props.get(c, "")
            for pn in PART_COLUMNS:     row[pn] = parts.get(pn, "")

            if should_keep_row(row, max_km, min_year):
                append_row_unique(daily_path, row, ALL_COLUMNS, unique_key="Link")
                append_row_to_existing_template(fixed_full_path, row, unique_key="Link")
                total_kept += 1
                print(f"ğŸ’¾ Kaydedildi: {row['BaÅŸlÄ±k'] or '(baÅŸlÄ±k yok)'}")
            else:
                print("ğŸš« Filtre nedeniyle eklenmedi.")

            time.sleep(random.uniform(3.0, 7.0))

        if stop_all:
            break

    print(f"\nâœ… AlÄ±nan ve filtreyi geÃ§en ilan: {total_kept}")
    print(f"ğŸ“„ GÃ¼nlÃ¼k dosya: {daily_path}")
    print(f"ğŸ“¦ Åablon dosya: {fixed_full_path}")

# ========= CLI =========
def ask_int(prompt: str) -> Optional[int]:
    s = input(prompt).strip()
    if not s: return None
    try: return int(s)
    except: return None

def main():
    ap = argparse.ArgumentParser(description="arabam.com â€” canonical + sort=startedAt.desc ile Ã§ek, filtrele, iki dosyaya yaz (gÃ¼nlÃ¼k + sabit ÅŸablon).")
    ap.add_argument("--query", type=str, default=None, help="Arama (Ã¶rn: 'volkswagen passat' ya da 'passat')")
    ap.add_argument("--min-year", type=int, default=None, help="En dÃ¼ÅŸÃ¼k model yÄ±lÄ±")
    ap.add_argument("--max-km", type=int, default=None, help="En yÃ¼ksek KM")
    ap.add_argument("--today", type=str, default=None, help="Hedef gÃ¼n (YYYY-MM-DD veya DD.MM.YYYY)")
    ap.add_argument("--max-pages", type=int, default=None, help="Maks. sayfa (opsiyonel)")
    args = ap.parse_args()

    q = args.query or input("Marka + model: ").strip()
    min_year = args.min_year if args.min_year is not None else ask_int("En dÃ¼ÅŸÃ¼k model yÄ±lÄ± (boÅŸ): ")
    max_km   = args.max_km   if args.max_km   is not None else ask_int("En yÃ¼ksek KM (boÅŸ): ")

    scrape(q, max_km=max_km, min_year=min_year, today_arg=args.today, max_pages=args.max_pages)

if __name__ == "__main__":
    main()
